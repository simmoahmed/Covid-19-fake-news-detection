{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5409,
     "status": "ok",
     "timestamp": 1607524656766,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "SlKM_pbxY817",
    "outputId": "0f05a0e0-59ea-499f-95b4-b352246712b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5397,
     "status": "ok",
     "timestamp": 1607524656767,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "D5sUgcJL8xtz",
    "outputId": "90a12eee-3a18-4abe-a918-b36dc735118a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8613,
     "status": "ok",
     "timestamp": 1607524659993,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "ObN9DKvpvZQa",
    "outputId": "d4ec251a-b0ac-4d4a-b4e9-f6b6de83a3ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14871,
     "status": "ok",
     "timestamp": 1607524666261,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "dNwTQef3Zs_-",
    "outputId": "488b2f3f-c586-4470-9865-821ebb3a765c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.32)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.94)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.32 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.19.32)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.11.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.32->boto3->pytorch-transformers) (2.8.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uP3tG3qRZvja"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import preprocessor as p\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, DebertaTokenizer, DebertaForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 17383,
     "status": "ok",
     "timestamp": 1607524668789,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "QYohf8eQZvmI",
    "outputId": "84b82d09-1872-4447-e8a4-e40568c94414"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-9BIVc_aJb3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Constraint_Train.csv\")\n",
    "val_df = pd.read_csv(\"../data/Constraint_Val.csv\")\n",
    "test_df = pd.read_csv(\"../data/Constraint_Test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17370,
     "status": "ok",
     "timestamp": 1607524668790,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "FRDWTp9mhJWl",
    "outputId": "1081bed8-5bae-4d90-819a-60f710481962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOO9UH0GvVSB"
   },
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "  text = row['tweet']\n",
    "  # text = text.strip('\\xa0')\n",
    "  text = p.clean(text)\n",
    "  # text = re.sub(r'\\([0-9]+\\)', '', text).strip()    \n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnpJ27eIvVU2"
   },
   "outputs": [],
   "source": [
    "# df['tweet'] = df.apply(lambda x: preprocess(x), 1)\n",
    "# val_df['tweet'] = val_df.apply(lambda x: preprocess(x), 1)\n",
    "# test_df['tweet'] = test_df.apply(lambda x: preprocess(x), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 17350,
     "status": "ok",
     "timestamp": 1607524668791,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "PmDvP8tSaJe4",
    "outputId": "fdd07cd3-0439-4b19-b7a8-358852e7f80d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 17340,
     "status": "ok",
     "timestamp": 1607524668792,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "JOEiX_vZwXXP",
    "outputId": "926ce80d-1b31-45ba-f435-67719f038949"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'States reported 1121 deaths a small rise from last Tuesday. Southern states reported 640 of those deaths. https://t.co/YASGRTT4ux'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "executionInfo": {
     "elapsed": 17330,
     "status": "ok",
     "timestamp": 1607524668792,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "NmlxkjFcjeX5",
    "outputId": "892ec32e-fea5-4e4f-bb72-97dc2a0af786"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet\n",
       "0   1  Our daily update is published. States reported...\n",
       "1   2             Alfalfa is the only cure for COVID-19."
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dHcpDJmbgaK"
   },
   "outputs": [],
   "source": [
    "def map_label(row):\n",
    "  return 0 if row['label']=='real' else 1\n",
    "\n",
    "df['label_encoded'] = df.apply(lambda x: map_label(x), 1)\n",
    "val_df['label_encoded'] = val_df.apply(lambda x: map_label(x), 1)\n",
    "# test_df['label_encoded'] = test_df.apply(lambda x: map_label(x), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkGKyR6ki-QQ"
   },
   "outputs": [],
   "source": [
    "train_sentences = df.tweet.values\n",
    "train_token_ids = df.id.values\n",
    "val_sentences = val_df.tweet.values\n",
    "val_token_ids = val_df.id.values\n",
    "test_sentences = test_df.tweet.values\n",
    "test_token_ids = test_df.id.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbjBqac2auaU"
   },
   "outputs": [],
   "source": [
    "train_sentences = [sentence + \" [SEP] [CLS]\" for sentence in train_sentences]\n",
    "train_labels = df.label_encoded.values\n",
    "val_sentences = [sentence + \" [SEP] [CLS]\" for sentence in val_sentences]\n",
    "val_labels = val_df.label_encoded.values\n",
    "test_sentences = [sentence + \" [SEP] [CLS]\" for sentence in test_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY1DD0vgG5qY"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYVUc8JiDxZX"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(network, train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids):\n",
    "  dic = {\"XLNET\": \"xlnet-base-cased\", \"ROBERT\": \"roberta-base\", \"XLM-ROBERT\": \"xlm-roberta-base\", \"DeBERTa\": \"microsoft/deberta-base\", \"Electra\": \"google/electra-base-discriminator\"}\n",
    "  if network=='XLNET':\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
    "  elif network=='DeBERTa':\n",
    "    tokenizer = DebertaTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
    "  elif network=='Electra':\n",
    "    tokenizer = ElectraTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
    "  else:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(dic[network], do_lower_case=True)\n",
    "\n",
    "  tokenized_train_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
    "  print (\"Tokenize the first sentence:\")\n",
    "  print (tokenized_train_texts[0])\n",
    "\n",
    "  tokenized_val_texts = [tokenizer.tokenize(sent) for sent in val_sentences]\n",
    "  tokenized_test_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "  input_train_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train_texts]\n",
    "  input_val_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_val_texts]\n",
    "  input_test_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test_texts]\n",
    "\n",
    "  input_train_ids = pad_sequences(input_train_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "  input_val_ids = pad_sequences(input_val_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "  input_test_ids = pad_sequences(input_test_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "  # Create attention masks\n",
    "  train_attention_masks = []\n",
    "\n",
    "  # Create a mask of 1s for each token followed by 0s for padding\n",
    "  for seq in input_train_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    train_attention_masks.append(seq_mask)\n",
    "  train_masks = np.array(train_attention_masks)\n",
    "\n",
    "  # Create attention masks\n",
    "  val_attention_masks = []\n",
    "\n",
    "  # Create a mask of 1s for each token followed by 0s for padding\n",
    "  for seq in input_val_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    val_attention_masks.append(seq_mask)\n",
    "  validation_masks = np.array(val_attention_masks)\n",
    "\n",
    "  # Create attention masks\n",
    "  test_attention_masks = []\n",
    "\n",
    "  # Create a mask of 1s for each token followed by 0s for padding\n",
    "  for seq in input_test_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)\n",
    "  test_masks = np.array(test_attention_masks)\n",
    "\n",
    "  train_inputs = torch.tensor(input_train_ids)\n",
    "  validation_inputs = torch.tensor(input_val_ids)\n",
    "  test_inputs = torch.tensor(input_test_ids)\n",
    "  train_labels = torch.tensor(train_labels)\n",
    "  validation_labels = torch.tensor(val_labels)\n",
    "  train_masks = torch.tensor(train_masks)\n",
    "  validation_masks = torch.tensor(validation_masks)\n",
    "  test_masks = torch.tensor(test_masks)\n",
    "\n",
    "  train_data = TensorDataset(torch.tensor(train_token_ids), train_inputs, train_masks, train_labels)\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "  validation_data = TensorDataset(torch.tensor(val_token_ids), validation_inputs, validation_masks, validation_labels)\n",
    "  validation_sampler = SequentialSampler(validation_data)\n",
    "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "  test_data = TensorDataset(torch.tensor(test_token_ids), test_inputs, test_masks)\n",
    "  test_sampler = SequentialSampler(test_data)\n",
    "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "  return train_dataloader, validation_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24075,
     "status": "ok",
     "timestamp": 1607524675569,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "aTYbjspkFZPg",
    "outputId": "73d0bd38-fdf3-446a-8e56-d85230398ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁the', '▁c', 'd', 'c', '▁currently', '▁reports', '▁9', '90', '31', '▁deaths', '.', '▁in', '▁general', '▁the', '▁discrepancies', '▁in', '▁death', '▁counts', '▁between', '▁different', '▁sources', '▁are', '▁small', '▁and', '▁', 'exp', 'lic', 'able', '.', '▁the', '▁death', '▁toll', '▁stands', '▁at', '▁roughly', '▁100', '000', '▁people', '▁today', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_XLNET, val_dataloader_XLNET, test_dataloader_XLNET = get_dataloader(\"XLNET\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28786,
     "status": "ok",
     "timestamp": 1607524680291,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "tciT_ZVAFZR4",
    "outputId": "e41a4459-a08b-4ef0-95b2-33d359008ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['The', 'ĠCDC', 'Ġcurrently', 'Ġreports', 'Ġ99', '031', 'Ġdeaths', '.', 'ĠIn', 'Ġgeneral', 'Ġthe', 'Ġdiscrepancies', 'Ġin', 'Ġdeath', 'Ġcounts', 'Ġbetween', 'Ġdifferent', 'Ġsources', 'Ġare', 'Ġsmall', 'Ġand', 'Ġexpl', 'icable', '.', 'ĠThe', 'Ġdeath', 'Ġtoll', 'Ġstands', 'Ġat', 'Ġroughly', 'Ġ100', '000', 'Ġpeople', 'Ġtoday', '.', 'Ġ[', 'SE', 'P', ']', 'Ġ[', 'CL', 'S', ']']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_ROBERT, val_dataloader_ROBERT, test_dataloader_ROBERT = get_dataloader(\"ROBERT\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36381,
     "status": "ok",
     "timestamp": 1607524687895,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "yY6szamxFZUR",
    "outputId": "11e8761e-095d-4646-f670-b1f6be2d3a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁The', '▁CD', 'C', '▁currently', '▁reports', '▁', '990', '31', '▁death', 's', '.', '▁In', '▁general', '▁the', '▁disc', 're', 'pa', 'ncies', '▁in', '▁death', '▁count', 's', '▁between', '▁different', '▁sources', '▁are', '▁small', '▁and', '▁explica', 'ble', '.', '▁The', '▁death', '▁toll', '▁stand', 's', '▁at', '▁rough', 'ly', '▁10', '0000', '▁people', '▁today', '.', '▁[', 'S', 'EP', ']', '▁[', 'C', 'LS', ']']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_XLM_ROBERT, val_dataloader_XLM_ROBERT, test_dataloader_XLM_ROBERT = get_dataloader(\"XLM-ROBERT\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44430,
     "status": "ok",
     "timestamp": 1607524695954,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "Va1PmzbsvShh",
    "outputId": "cf366638-8dba-44e5-b868-f5a545a68168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['1169', '269', '17896', '3058', '3136', '7388', '43637', '7040', '13', '287', '2276', '262', '42420', '287', '1918', '9853', '1022', '1180', '4237', '389', '1402', '290', '1193', '18424', '13', '262', '1918', '13592', '6296', '379', '7323', '1802', '830', '661', '1909', '13', '[SEP]', '[CLS]']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_DeBERTa, val_dataloader_DeBERTa, test_dataloader_DeBERTa = get_dataloader(\"DeBERTa\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3T1NWky1ccw"
   },
   "outputs": [],
   "source": [
    "# train_dataloader_Electra, val_dataloader_Electra, test_dataloader_Electra = get_dataloader(\"Electra\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121370,
     "status": "ok",
     "timestamp": 1607524772909,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "DYrDZUkI_YMx",
    "outputId": "fdc668c3-d78d-425a-f5a2-cf24a5322145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = RobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
    "directory_path = '/content/drive/MyDrive/Constraint/models/Best models'\n",
    "model1.load_state_dict(torch.load(directory_path+'/XLM-ROBERTa_base_preprocess_link_v1.ckpt'))\n",
    "model1.eval()\n",
    "model1.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154250,
     "status": "ok",
     "timestamp": 1607524805797,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "X5jxWyf2LBnH",
    "outputId": "80f4d669-fa5e-407c-b7b0-52410279968b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "directory_path = '/content/drive/MyDrive/Constraint/models/Best models'\n",
    "model2.load_state_dict(torch.load(directory_path+'/ROBERTa_base_preprocess_v2.ckpt'))\n",
    "model2.eval()\n",
    "model2.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185168,
     "status": "ok",
     "timestamp": 1607524836723,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "boZzF_rrLBqH",
    "outputId": "70f7fd0e-c78b-4dbd-fd79-453e5591c1cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
    "directory_path = '/content/drive/MyDrive/Constraint/models/Best models'\n",
    "model3.load_state_dict(torch.load(directory_path+'/XLNet_base_cased_v2.ckpt'))\n",
    "model3.eval()\n",
    "model3.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222115,
     "status": "ok",
     "timestamp": 1607524873678,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "Bdfib12CsRZ_",
    "outputId": "7c0b170e-403a-409a-a865-c1e01070fcfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'config', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaForSequenceClassification(\n",
       "  (deberta): DebertaModel(\n",
       "    (embeddings): DebertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
       "      (LayerNorm): DebertaLayerNorm()\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (1): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (2): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (3): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (4): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (5): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (6): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (7): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (8): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (9): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (10): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (11): DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(1024, 768)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=2)\n",
    "directory_path = '/content/drive/MyDrive/Constraint/models/Best models'\n",
    "model4.load_state_dict(torch.load(directory_path+'/DeBERTa_base_preprocess_link_v1.ckpt'))\n",
    "model4.eval()\n",
    "model4.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlS3R5VBEdif"
   },
   "outputs": [],
   "source": [
    "# model5 = ElectraForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=2)\n",
    "# directory_path = '/content/drive/MyDrive/Constraint/models/Best models'\n",
    "# model5.load_state_dict(torch.load(directory_path+'/Electra_base_0.973414_preprocess_model.ckpt'))\n",
    "# model5.eval()\n",
    "# model5.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXy7-mV8Ew0T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bpvMMeaeskY"
   },
   "outputs": [],
   "source": [
    "def get_model_preds_labels(model, dataloader, mode='train'):\n",
    "  data_vectors = []\n",
    "  labels = []\n",
    "  ids = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for i, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        if mode=='test':\n",
    "            token_ids, b_input_ids, b_input_mask = batch\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            for point, tk_id in zip(F.softmax(outputs.logits).tolist(), token_ids.tolist()):\n",
    "              data_vectors.append(point)       \n",
    "              ids.append(tk_id)     \n",
    "        else:\n",
    "          # Unpack the inputs from our dataloader\n",
    "          token_ids, b_input_ids, b_input_mask, b_labels = batch\n",
    "          # Forward pass\n",
    "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "          for point, lab, tk_id in zip(F.softmax(outputs.logits).tolist(), b_labels.tolist(), token_ids.tolist()):\n",
    "            data_vectors.append(point)\n",
    "            labels.append(lab)\n",
    "            ids.append(tk_id)\n",
    "\n",
    "  if mode=='test':\n",
    "    return data_vectors, ids\n",
    "  else:\n",
    "    return data_vectors, labels, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245952,
     "status": "ok",
     "timestamp": 1607524897540,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "8sMP2x39eswb",
    "outputId": "6781322e-900e-4a86-8e58-1dbf66b0abdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "m1, lb, train_ids1 = get_model_preds_labels(model1, train_dataloader_XLM_ROBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270150,
     "status": "ok",
     "timestamp": 1607524921747,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "ln3CNuPWfcU_",
    "outputId": "5db827a3-499a-4d35-f93c-373792c3d169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "m2, _, train_ids2 = get_model_preds_labels(model2, train_dataloader_ROBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306271,
     "status": "ok",
     "timestamp": 1607524957875,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "cBjZxuC2fcXQ",
    "outputId": "1069d5a2-4934-4a78-b844-2851b01b26b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "m3, _, train_ids3 = get_model_preds_labels(model3, train_dataloader_XLNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337360,
     "status": "ok",
     "timestamp": 1607524988972,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "g6-EmAUCvu0J",
    "outputId": "a556838e-591d-440a-ac18-6c0c5a222ffd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "m4, _, train_ids4 = get_model_preds_labels(model4, train_dataloader_DeBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iijTUuDf1pxY"
   },
   "outputs": [],
   "source": [
    "# m5, _, train_ids5 = get_model_preds_labels(model5, train_dataloader_Electra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fqj-bhqg7iD"
   },
   "outputs": [],
   "source": [
    "a = [each[0] for each in m1]\n",
    "b = [each[1] for each in m1]\n",
    "train_df = pd.DataFrame({'id': train_ids1, 'model1_Class0': a, 'model1_Class1': b})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57Cx9WXpfWJi"
   },
   "outputs": [],
   "source": [
    "def create_dataset(df, ids, model_op, col1, col2):\n",
    "  a = [each[0] for each in model_op]\n",
    "  b = [each[1] for each in model_op]\n",
    "  new_df = pd.DataFrame({'id': ids, col1: a, col2: b})\n",
    "  df = df.merge(new_df, on='id', how='left')\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBSn4cr1KMm3"
   },
   "outputs": [],
   "source": [
    "train_df = create_dataset(train_df,train_ids2,m2, 'model2_Class0', 'model2_Class1')\n",
    "train_df = create_dataset(train_df,train_ids3,m3, 'model3_Class0', 'model3_Class1')\n",
    "train_df = create_dataset(train_df,train_ids4,m4, 'model4_Class0', 'model4_Class1')\n",
    "# train_df = create_dataset(train_df,train_ids4,m4, 'model5_Class0', 'model5_Class1')\n",
    "train_df = train_df.merge(pd.DataFrame({'id': train_ids1, 'Label': lb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337336,
     "status": "ok",
     "timestamp": 1607524988978,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "4kcwQgyON4D_",
    "outputId": "96fedc9e-fb5b-482d-85ee-bfd07fdd851c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6420, 10), Index(['id', 'model1_Class0', 'model1_Class1', 'model2_Class0',\n",
       "        'model2_Class1', 'model3_Class0', 'model3_Class1', 'model4_Class0',\n",
       "        'model4_Class1', 'Label'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 337326,
     "status": "ok",
     "timestamp": 1607524988979,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "vLeuLs7DKMpU",
    "outputId": "35b7827f-611d-491f-c5f5-a6e83b710771"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model1_Class0</th>\n",
       "      <th>model1_Class1</th>\n",
       "      <th>model2_Class0</th>\n",
       "      <th>model2_Class1</th>\n",
       "      <th>model3_Class0</th>\n",
       "      <th>model3_Class1</th>\n",
       "      <th>model4_Class0</th>\n",
       "      <th>model4_Class1</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.999035</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999843</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6415</th>\n",
       "      <td>6416</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.998776</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.993787</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6416</th>\n",
       "      <td>6417</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.999087</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6417</th>\n",
       "      <td>6418</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.999355</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.999075</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6418</th>\n",
       "      <td>6419</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6419</th>\n",
       "      <td>6420</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6420 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  model1_Class0  model1_Class1  ...  model4_Class0  model4_Class1  Label\n",
       "0        1       0.999289       0.000711  ...       0.999958       0.000042      0\n",
       "1        2       0.999703       0.000297  ...       0.999892       0.000108      0\n",
       "2        3       0.000418       0.999582  ...       0.000300       0.999700      1\n",
       "3        4       0.999864       0.000136  ...       0.999961       0.000039      0\n",
       "4        5       0.999843       0.000157  ...       0.999956       0.000044      0\n",
       "...    ...            ...            ...  ...            ...            ...    ...\n",
       "6415  6416       0.001224       0.998776  ...       0.000114       0.999886      1\n",
       "6416  6417       0.000384       0.999616  ...       0.000053       0.999947      1\n",
       "6417  6418       0.000645       0.999355  ...       0.000077       0.999923      1\n",
       "6418  6419       0.000401       0.999600  ...       0.000459       0.999541      1\n",
       "6419  6420       0.999719       0.000281  ...       0.999941       0.000059      0\n",
       "\n",
       "[6420 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(train_X, columns = [\"model1_Class0\", \"model1_Class1\", \"model2_Class0\", \"model2_Class1\", \"model3_Class0\", \"model3_Class1\", \"model4_Class0\", \"model4_Class1\"])\n",
    "# train_df['Label'] = train_y\n",
    "train_df = train_df.sort_values(by='id').reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGaJibCQKMse"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('Boosting Data/Train_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0449EoUaKMvB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346104,
     "status": "ok",
     "timestamp": 1607524997775,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "L790tlfxN-bH",
    "outputId": "b89b47d0-1cbd-40cf-da1e-2fb2a820d91f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "v1, v_lb, v_ids1 = get_model_preds_labels(model1, val_dataloader_XLM_ROBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353930,
     "status": "ok",
     "timestamp": 1607525005608,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "1QmRpNKEN-dz",
    "outputId": "bbd4d3d6-96d6-4dc3-e28b-ae42eb704f39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "v2, _, v_ids2 = get_model_preds_labels(model2, val_dataloader_ROBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366052,
     "status": "ok",
     "timestamp": 1607525017738,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "2yGpY8veN-go",
    "outputId": "fe531932-4d2b-473c-81cf-118f8947b04e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "v3, _, v_ids3 = get_model_preds_labels(model3, val_dataloader_XLNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376112,
     "status": "ok",
     "timestamp": 1607525027806,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "H_sKaCniwvtw",
    "outputId": "6e2be95a-38d9-4b99-a0f0-59cf02ba3a4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "v4, _, v_ids4 = get_model_preds_labels(model4, val_dataloader_DeBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NScapKFS16oi"
   },
   "outputs": [],
   "source": [
    "# v5, _, v_ids5 = get_model_preds_labels(model5, val_dataloader_Electra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAUW3l91qw_y"
   },
   "outputs": [],
   "source": [
    "a = [each[0] for each in v1]\n",
    "b = [each[1] for each in v1]\n",
    "validation_df = pd.DataFrame({'id': v_ids1, 'model1_Class0': a, 'model1_Class1': b})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ3Q0vrFN-jM"
   },
   "outputs": [],
   "source": [
    "validation_df = create_dataset(validation_df, v_ids2, v2, 'model2_Class0', 'model2_Class1')\n",
    "validation_df = create_dataset(validation_df, v_ids3, v3, 'model3_Class0', 'model3_Class1')\n",
    "validation_df = create_dataset(validation_df, v_ids4, v4, 'model4_Class0', 'model4_Class1')\n",
    "# validation_df = create_dataset(validation_df, v_ids5, v5, 'model5_Class0', 'model5_Class1')\n",
    "validation_df = validation_df.merge(pd.DataFrame({'id': v_ids1, 'Label': v_lb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376090,
     "status": "ok",
     "timestamp": 1607525027808,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "m_adcfoUN-lU",
    "outputId": "93ff0746-2eaa-4d7c-ae54-7c8c1d55917f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2140, 10), Index(['id', 'model1_Class0', 'model1_Class1', 'model2_Class0',\n",
       "        'model2_Class1', 'model3_Class0', 'model3_Class1', 'model4_Class0',\n",
       "        'model4_Class1', 'Label'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape, validation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 376084,
     "status": "ok",
     "timestamp": 1607525027810,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "i_P5msVwN-nt",
    "outputId": "47497a82-68e0-42fa-c5cd-af18c3001c02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model1_Class0</th>\n",
       "      <th>model1_Class1</th>\n",
       "      <th>model2_Class0</th>\n",
       "      <th>model2_Class1</th>\n",
       "      <th>model3_Class0</th>\n",
       "      <th>model3_Class1</th>\n",
       "      <th>model4_Class0</th>\n",
       "      <th>model4_Class1</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.998731</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.991674</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.989981</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.997282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.999615</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.999065</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.998732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.999052</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.999784</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.999457</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.999087</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.995166</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.951092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.981343</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.998876</td>\n",
       "      <td>0.029504</td>\n",
       "      <td>0.970496</td>\n",
       "      <td>0.920868</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.999064</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.991048</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  model1_Class0  model1_Class1  ...  model4_Class0  model4_Class1  Label\n",
       "0        1       0.001269       0.998731  ...       0.028954       0.971046      1\n",
       "1        2       0.991674       0.008326  ...       0.002718       0.997282      1\n",
       "2        3       0.000385       0.999615  ...       0.000053       0.999947      1\n",
       "3        4       0.000416       0.999584  ...       0.001268       0.998732      1\n",
       "4        5       0.999851       0.000149  ...       0.999951       0.000049      0\n",
       "...    ...            ...            ...  ...            ...            ...    ...\n",
       "2135  2136       0.000510       0.999490  ...       0.001650       0.998350      1\n",
       "2136  2137       0.999858       0.000142  ...       0.999962       0.000037      0\n",
       "2137  2138       0.004834       0.995166  ...       0.048908       0.951092      1\n",
       "2138  2139       0.018657       0.981343  ...       0.920868       0.079132      1\n",
       "2139  2140       0.999850       0.000150  ...       0.999961       0.000039      0\n",
       "\n",
       "[2140 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = validation_df.sort_values(by='id').reset_index(drop=True)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtTV8i-jN-ql"
   },
   "outputs": [],
   "source": [
    "validation_df.to_csv('Boosting Data/Validation_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP8ptYWxOkUA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384801,
     "status": "ok",
     "timestamp": 1607525036542,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "YSMv3dbJbBYc",
    "outputId": "9e2a3354-9202-4556-f78e-44314d99986f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t1, t_ids1 = get_model_preds_labels(model1, test_dataloader_XLM_ROBERT, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392756,
     "status": "ok",
     "timestamp": 1607525044505,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "BZxzQ9unbBbV",
    "outputId": "36b9b2d9-8794-45a3-d944-f11c56feca17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t2, t_ids2 = get_model_preds_labels(model2, test_dataloader_ROBERT, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404793,
     "status": "ok",
     "timestamp": 1607525056549,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "gctMxAf1bBeV",
    "outputId": "fb90b1ad-5635-4c0d-8f69-610d9692bb95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t3, t_ids3 = get_model_preds_labels(model3, test_dataloader_XLNET, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415118,
     "status": "ok",
     "timestamp": 1607525066882,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "AdphIQHfwoAw",
    "outputId": "307e5c09-03ac-491c-bd54-5ce0b00d00a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "t4, t_ids4 = get_model_preds_labels(model4, test_dataloader_DeBERTa, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9lxIiHq2Kdu"
   },
   "outputs": [],
   "source": [
    "# t5, t_ids5 = get_model_preds_labels(model5, test_dataloader_Electra, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jyaJ79SbBgg"
   },
   "outputs": [],
   "source": [
    "a = [each[0] for each in t1]\n",
    "b = [each[1] for each in t1]\n",
    "test_df = pd.DataFrame({'id': t_ids1, 'model1_Class0': a, 'model1_Class1': b})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM7CbMbWesy1"
   },
   "outputs": [],
   "source": [
    "test_df = create_dataset(test_df, t_ids2, t2, 'model2_Class0', 'model2_Class1')\n",
    "test_df = create_dataset(test_df,  t_ids3, t3, 'model3_Class0', 'model3_Class1')\n",
    "test_df = create_dataset(test_df, t_ids4, t4, 'model4_Class0', 'model4_Class1')\n",
    "# test_df = create_dataset(test_df, t_ids5, t5, 'model5_Class0', 'model5_Class1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415097,
     "status": "ok",
     "timestamp": 1607525066884,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "04f35NZCtSpK",
    "outputId": "795c2e39-5f64-48e2-873c-f2a317579464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2140, 9), Index(['id', 'model1_Class0', 'model1_Class1', 'model2_Class0',\n",
       "        'model2_Class1', 'model3_Class0', 'model3_Class1', 'model4_Class0',\n",
       "        'model4_Class1'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape, test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 415089,
     "status": "ok",
     "timestamp": 1607525066885,
     "user": {
      "displayName": "Ayan Basak",
      "photoUrl": "",
      "userId": "03457614456337947866"
     },
     "user_tz": -330
    },
    "id": "3Jt5UbZWtSsh",
    "outputId": "fa5671b8-17b8-4936-947c-e1e5e4cc442d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model1_Class0</th>\n",
       "      <th>model1_Class1</th>\n",
       "      <th>model2_Class0</th>\n",
       "      <th>model2_Class1</th>\n",
       "      <th>model3_Class0</th>\n",
       "      <th>model3_Class1</th>\n",
       "      <th>model4_Class0</th>\n",
       "      <th>model4_Class1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.999578</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.999830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.999082</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2136</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.999020</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2137</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.999095</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.999961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>2138</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.999084</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.999925</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>2139</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.999778</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.999931</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>2140</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.887354</td>\n",
       "      <td>0.112646</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  model1_Class0  ...  model4_Class0  model4_Class1\n",
       "0        1       0.999851  ...       0.999963       0.000037\n",
       "1        2       0.000422  ...       0.000112       0.999888\n",
       "2        3       0.000401  ...       0.000170       0.999830\n",
       "3        4       0.999858  ...       0.999956       0.000044\n",
       "4        5       0.999852  ...       0.999912       0.000088\n",
       "...    ...            ...  ...            ...            ...\n",
       "2135  2136       0.999821  ...       0.999965       0.000035\n",
       "2136  2137       0.000393  ...       0.000039       0.999961\n",
       "2137  2138       0.999857  ...       0.999950       0.000050\n",
       "2138  2139       0.999844  ...       0.999931       0.000069\n",
       "2139  2140       0.999440  ...       0.999542       0.000458\n",
       "\n",
       "[2140 rows x 9 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.sort_values(by='id').reset_index(drop=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG7s4Eb8b_ae"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('Boosting Data/Test_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0FGTddX2_kT"
   },
   "outputs": [],
   "source": [
    "model1: \"xlm-robert\", model2: \"robert\", model3: \"XLNET\", model4: \"DeBERTa\", model5: \"Electra\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLCFhQ9tgrY4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Boosting_Data_Creation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
